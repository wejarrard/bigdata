{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Project: H&M Personalized Fashion Recommendations\n",
    "#### Market Basket Analysis (MBA) Preprocessing\n",
    "#### Will Jarrard (wej5ar) Abhi Dommalapati (ad4bu), Sebastian Ranasinghe (sar2jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/h_and_m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%cd /project/ds5559/h_and_m/\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import collect_set, col, count, row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, year, month, col, date_format\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"100g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.csv('/project/ds5559/h_and_m/customers.csv',  inferSchema=True, header = True)\n",
    "articles = spark.read.csv('/project/ds5559/h_and_m/articles.csv',  inferSchema=True, header = True)\n",
    "transactions_full = spark.read.csv('/project/ds5559/h_and_m/transactions_train.csv',  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "|     t_dat|         customer_id|article_id|               price|sales_channel_id|\n",
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "|2018-09-20|000058a12d5b43e67...| 663713001|0.050830508474576264|               2|\n",
      "|2018-09-20|000058a12d5b43e67...| 541518023| 0.03049152542372881|               2|\n",
      "|2018-09-20|00007d2de826758b6...| 505221004| 0.01523728813559322|               2|\n",
      "|2018-09-20|00007d2de826758b6...| 685687003|0.016932203389830508|               2|\n",
      "|2018-09-20|00007d2de826758b6...| 685687004|0.016932203389830508|               2|\n",
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_full.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Lookup Table for Article Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distinct pairings of product types, and index\n",
    "\n",
    "art_lookup = articles.select(col('product_type_name'), col('index_name')) \\\n",
    "                       .distinct() \\\n",
    "                       .sort(col('product_type_no'))\n",
    "\n",
    "\n",
    "# Create ID for unique product, index types\n",
    "\n",
    "w = Window().orderBy(lit('A'))\n",
    "art_lookup = art_lookup.withColumn(\"prod_index_id\", row_number().over(w))\n",
    "\n",
    "\n",
    "# Rejoin with articles, extract relevant columns\n",
    "\n",
    "art_lookup = articles.join(art_lookup, \n",
    "                           on = ['product_type_name', 'index_name'], \n",
    "                           how = 'inner')\n",
    "\n",
    "art_lookup = art_lookup.select(col('article_id'),\n",
    "                  col('product_type_name'), col('product_code'),\n",
    "                  col('product_type_no'), \n",
    "                  col('index_name'), \n",
    "                  col('prod_index_id')).sort(col('prod_index_id'))\n",
    "\n",
    "art_lookup_short = art_lookup.select(col('article_id'), \n",
    "                  col('prod_index_id')).sort(col('prod_index_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|article_id|prod_index_id|\n",
      "+----------+-------------+\n",
      "| 519243001|            1|\n",
      "| 919896001|            2|\n",
      "| 894221001|            2|\n",
      "| 902265001|            2|\n",
      "| 902265002|            2|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Lookup Table\n",
    "\n",
    "art_lookup_short.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+--------------------+-------------+\n",
      "|product_type_name|product_type_no|          index_name|prod_index_id|\n",
      "+-----------------+---------------+--------------------+-------------+\n",
      "|            Shirt|            259|Children Sizes 13...|          261|\n",
      "|            Shirt|            259|    Baby Sizes 50-98|          262|\n",
      "|            Shirt|            259|             Divided|          263|\n",
      "|            Shirt|            259|Children Sizes 92...|          264|\n",
      "|            Shirt|            259|          Ladieswear|          265|\n",
      "|            Shirt|            259|            Menswear|          266|\n",
      "|            Shirt|            259|Children Accessor...|          267|\n",
      "+-----------------+---------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See that shirts now have a unique ID for each index\n",
    "\n",
    "art_lookup.select(col('product_type_name'), \n",
    "                col('product_type_no'), \n",
    "                col('index_name'), \n",
    "                col('prod_index_id')).distinct().sort(col('prod_index_id')).filter(col('product_type_name') == 'Shirt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To reduce size of dataset, only orders from 2020 will be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+----------------+-------------------+----+\n",
      "|     t_dat|         customer_id|article_id|               price|sales_channel_id|               date|year|\n",
      "+----------+--------------------+----------+--------------------+----------------+-------------------+----+\n",
      "|2020-03-21|00000dbacae5abe5e...| 887593002| 0.02159322033898305|               2|2020-03-21 00:00:00|2020|\n",
      "|2020-03-21|00000dbacae5abe5e...| 795440001|0.014389830508474576|               2|2020-03-21 00:00:00|2020|\n",
      "|2020-03-21|00000dbacae5abe5e...| 841260003|0.011508474576271186|               2|2020-03-21 00:00:00|2020|\n",
      "|2020-03-21|00000dbacae5abe5e...| 859416011|0.014389830508474576|               2|2020-03-21 00:00:00|2020|\n",
      "|2020-03-21|00000dbacae5abe5e...| 890498002|0.031762711864406774|               2|2020-03-21 00:00:00|2020|\n",
      "+----------+--------------------+----------+--------------------+----------------+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Used https://stackoverflow.com/questions/53285032/how-do-i-convert-timestamp-to-unix-format-with-pyspark\n",
    "# and https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.from_unixtime.html\n",
    "\n",
    "transactions_2020 =  transactions_full.withColumn('t_dat', transactions_full['t_dat'].cast('string'))\n",
    "transactions_2020 = transactions_2020.withColumn('date', from_unixtime(unix_timestamp('t_dat', 'yyyy-MM-dd')))\n",
    "transactions_2020 = transactions_2020.withColumn('year', year(col('date')))\n",
    "transactions_2020 = transactions_2020[transactions_2020['year'] == 2020]\n",
    "\n",
    "transactions_2020 = transactions_2020.sort(col('customer_id'))\n",
    "\n",
    "transactions_2020.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to parquet file and create temporary view\n",
    "\n",
    "transactions_2020.write.mode('overwrite').parquet(\"trans2020_par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create training set by subsetting 2020 transaction list by transactions that occurred on / before July 31, 2020. We will treat transactions that occurred on / after August 1, 2020 as the \"future\" and make predictions later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from parquet file created in above cell\n",
    "\n",
    "parquetFile = spark.read.parquet(\"trans2020_par\")\n",
    "parquetFile.createOrReplaceTempView(\"trans2020_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+----------------+-------------------+----+-------------+\n",
      "|article_id|     t_dat|         customer_id|               price|sales_channel_id|               date|year|prod_index_id|\n",
      "+----------+----------+--------------------+--------------------+----------------+-------------------+----+-------------+\n",
      "| 835417002|2020-03-02|03746d27db5ebfc0e...|0.042355932203389825|               2|2020-03-02 00:00:00|2020|          258|\n",
      "| 698286007|2020-06-26|03746d27db5ebfc0e...| 0.02240677966101695|               1|2020-06-26 00:00:00|2020|          415|\n",
      "| 559601019|2020-06-26|03746d27db5ebfc0e...|0.019423728813559322|               1|2020-06-26 00:00:00|2020|           21|\n",
      "| 543054016|2020-03-02|03746d27db5ebfc0e...|0.042355932203389825|               2|2020-03-02 00:00:00|2020|          215|\n",
      "| 863409002|2020-03-02|03746d27db5ebfc0e...|0.033881355932203386|               2|2020-03-02 00:00:00|2020|          258|\n",
      "| 673638007|2020-03-02|03746d27db5ebfc0e...|0.016932203389830508|               2|2020-03-02 00:00:00|2020|          216|\n",
      "| 847673002|2020-03-02|03746d27db5ebfc0e...|0.033881355932203386|               2|2020-03-02 00:00:00|2020|          258|\n",
      "| 850581002|2020-03-02|03746d27db5ebfc0e...|0.022016949152542376|               2|2020-03-02 00:00:00|2020|          215|\n",
      "| 752554004|2020-06-26|03746d27db5ebfc0e...|0.008966101694915254|               1|2020-06-26 00:00:00|2020|           90|\n",
      "| 816805002|2020-06-28|037479bc03bf931f2...| 0.01523728813559322|               2|2020-06-28 00:00:00|2020|          258|\n",
      "+----------+----------+--------------------+--------------------+----------------+-------------------+----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_train = spark.sql(\"SELECT * FROM trans2020_temp WHERE t_dat <= '2020-07-31' limit 5000000\")\n",
    "transaction_train = transaction_train.join(art_lookup_short, on = 'article_id', how = 'inner')\n",
    "\n",
    "transaction_train.sort(col('customer_id')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create temporary dataframe and then extract columns\n",
    "\n",
    "transaction_train.createOrReplaceTempView('train_temp')\n",
    "train_temp = spark.sql(\"select s.prod_index_id, s.customer_id \\\n",
    "                    from train_temp s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to parquet file\n",
    "\n",
    "train_temp.write.mode('overwrite').parquet(\"train_par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next the test set is created. As mentioned earlier, we are treating transactions that occurred on / after August 1, 2020 as the \"future\". \n",
    "##### We create a list of unique customers for which we want to predict on. A table is then created which shows by customer ID, the purchases made on / before July 31, 2020 (purchase history), the purchases made on / after August 1, 2020 (future purchases, aka labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp = spark.sql(\"SELECT * FROM trans2020_temp WHERE t_dat > '2020-07-31' limit 500000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of customers that made purchases on or after August 1, 2020\n",
    "\n",
    "customer_test = test_temp.select(col('customer_id')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the customer list onto the transactions_2020 dataset\n",
    "# Purpose: Filter to only transactions with these customers\n",
    "\n",
    "temp = spark.sql(\"SELECT * FROM trans2020_temp\")\n",
    "\n",
    "transaction_test = temp.join(customer_test, on = 'customer_id', how = 'leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join art_lookup_short to add the prod_idex_id\n",
    "\n",
    "transaction_test = transaction_test.join(art_lookup_short, on = 'article_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_test.createOrReplaceTempView(\"transaction_test_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baskets of items for before August 1, 2020\n",
    "\n",
    "transaction_test_prior = spark.sql(\"SELECT prod_index_id, customer_id FROM transaction_test_temp WHERE t_dat <= '2020-07-31'\")\n",
    "\n",
    "# Write to parquet file\n",
    "\n",
    "transaction_test_prior.write.mode('overwrite').parquet(\"testprior_par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baskets of items for on / after August 1, 2020\n",
    "\n",
    "transaction_test_after = spark.sql(\"SELECT article_id, customer_id FROM transaction_test_temp WHERE t_dat > '2020-07-31'\")\n",
    "\n",
    "# Write to parquet file\n",
    "\n",
    "transaction_test_after.write.mode('overwrite').parquet(\"testafter_par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         customer_id|              labels|\n",
      "+--------------------+--------------------+\n",
      "|04daaa60957b280cb...|[794575001, 62448...|\n",
      "|054324bf3c4451750...|         [806778001]|\n",
      "|058306b9a1720d1a0...|[924250001, 89616...|\n",
      "|05a45a5e4c53a72f1...|         [832481003]|\n",
      "|05a8260d130f082b0...|[716672001, 27038...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_baskets_after.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
